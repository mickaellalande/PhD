{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate the SWE/SCA HMASR to coarser resolutions with and without mask (mean and median)\n",
    "\n",
    "To test the drift\n",
    "\n",
    "- HMASR: [High Mountain Asia UCLA Daily Snow Reanalysis, Version 1](https://nsidc.org/data/HMA_SR_D)\n",
    "\n",
    "conda env: `phd_v3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 | packaged by conda-forge | (default, Jul 24 2020, 01:25:15) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "# To reload external files automatically (ex: utils)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar as cld\n",
    "import matplotlib.pyplot as plt\n",
    "import proplot as plot # New plot library (https://proplot.readthedocs.io/en/latest/)\n",
    "plot.rc['savefig.dpi'] = 300 # 1200 is too big! #https://proplot.readthedocs.io/en/latest/basics.html#Creating-figures\n",
    "from scipy import stats\n",
    "import xesmf as xe # For regridding (https://xesmf.readthedocs.io/en/latest/)\n",
    "\n",
    "import sys\n",
    "# sys.path.insert(1, '/home/mlalande/notebooks/utils') # to include my util file in previous directory\n",
    "sys.path.insert(1, '../utils') # to include my util file in previous directory\n",
    "import utils as u # my personal functions\n",
    "u.check_python_version()\n",
    "# u.check_virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:44181</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>24.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:44181' processes=4 threads=12, memory=24.00 GB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For parallelisation\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=4, threads_per_worker=3, memory_limit='6GB')\n",
    "# client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WY1999_00\n",
      "WY2000_01\n",
      "WY2001_02\n",
      "WY2002_03\n",
      "WY2003_04\n",
      "WY2004_05\n",
      "WY2005_06\n",
      "WY2006_07\n",
      "WY2007_08\n",
      "WY2008_09\n",
      "WY2009_10\n",
      "WY2010_11\n",
      "WY2011_12\n",
      "WY2012_13\n",
      "WY2013_14\n",
      "WY2014_15\n",
      "WY2015_16\n",
      "WY2016_17\n"
     ]
    }
   ],
   "source": [
    "# Create folders\n",
    "WY_list = ['WY'+str(year)+'_'+str(i).zfill(2) for i, year in enumerate(range(1999, 2017))]\n",
    "path_out = '/mnt/lalandmi/equipes/C2H/HMASR/HMASR_100km_mask/'\n",
    "\n",
    "for WY in WY_list:\n",
    "    print(WY)\n",
    "    !mkdir {path_out}{WY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FORCING_POST -> PPT_Post\n",
    "- SD_POST -> SD_Post\n",
    "- SWE_SCA_POST -> SWE_Post, SCA_Post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate data in parallel\n",
    "There are different option with `dask`. I didn't succeed to make it work with `xr.open_mfdataset` as there are too many files and there is a memory crash. So I finally used the `delayed` option from dask, in order to parallelize a loop.\n",
    "\n",
    "Here are the different ressources:\n",
    "- http://xarray.pydata.org/en/stable/user-guide/dask.html\n",
    "- https://ncar.github.io/xdev/posts/writing-multiple-netcdf-files-in-parallel-with-xarray-and-dask/ (I use this on for writting in parallel)\n",
    "- https://tutorial.dask.org/01_dask.delayed.html (I used this example for delayed)\n",
    "\n",
    "The delayed from dask is a bit tricky and we can't use all functions.\n",
    "\n",
    "The goal here is just to coarsen grids. Then I will put all the files together in a single file for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WY1999_00\n",
      "SWE_SCA_POST\n",
      "WY2000_01\n",
      "SWE_SCA_POST\n",
      "WY2001_02\n",
      "SWE_SCA_POST\n",
      "WY2002_03\n",
      "SWE_SCA_POST\n",
      "WY2003_04\n",
      "SWE_SCA_POST\n",
      "WY2004_05\n",
      "SWE_SCA_POST\n",
      "WY2005_06\n",
      "SWE_SCA_POST\n",
      "WY2006_07\n",
      "SWE_SCA_POST\n",
      "WY2007_08\n",
      "SWE_SCA_POST\n",
      "WY2008_09\n",
      "SWE_SCA_POST\n",
      "WY2009_10\n",
      "SWE_SCA_POST\n",
      "WY2010_11\n",
      "SWE_SCA_POST\n",
      "WY2011_12\n",
      "SWE_SCA_POST\n",
      "WY2012_13\n",
      "SWE_SCA_POST\n",
      "WY2013_14\n",
      "SWE_SCA_POST\n",
      "WY2014_15\n",
      "SWE_SCA_POST\n",
      "WY2015_16\n",
      "SWE_SCA_POST\n",
      "WY2016_17\n",
      "SWE_SCA_POST\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "WY_list = ['WY'+str(year)+'_'+str(i).zfill(2) for i, year in enumerate(range(1999, 2017))]\n",
    "# ds_name_list = ['FORCING_POST', 'SD_POST',  'SWE_SCA_POST']\n",
    "ds_name_list = ['SWE_SCA_POST']\n",
    "\n",
    "for WY in WY_list:\n",
    "    print(WY)\n",
    "    path = '/mnt/lalandmi/equipes/C2H/HMASR/HMA_SR_D/'+WY+'/'\n",
    "    path_out = '/mnt/lalandmi/equipes/C2H/HMASR/HMASR_100km_mask/'+WY+'/'\n",
    "    \n",
    "    for ds_name in ds_name_list:\n",
    "        print(ds_name)\n",
    "        list_files = [f for f in listdir(path) if ds_name in f]\n",
    "        \n",
    "        ds_100km = []\n",
    "        ds_100km_mask = []\n",
    "        for file in list_files:\n",
    "            ds = delayed(xr.open_dataset)(path+file)\n",
    "            mask = delayed(xr.open_dataset)(path+file.replace(ds_name, 'MASK'))\n",
    "            \n",
    "            if ds_name in ['FORCING_POST']:\n",
    "                ds_100km.append(ds.coarsen(Latitude=225, Longitude=225).mean())\n",
    "                ds_100km_mask.append(ds.where(mask.Non_seasonal_snow_mask == 0).coarsen(Latitude=225, Longitude=225).mean())\n",
    "            else:\n",
    "                # Select only the mean (Stats=0)\n",
    "                ds_100km.append(ds.isel(Stats=[0,2]).coarsen(Latitude=225, Longitude=225).mean())\n",
    "                ds_100km_mask.append(ds.isel(Stats=[0,2]).where(mask.Non_seasonal_snow_mask == 0).coarsen(Latitude=225, Longitude=225).mean())\n",
    "                \n",
    "        ds_100km = compute(ds_100km)\n",
    "        ds_100km_mask = compute(ds_100km_mask)\n",
    "        xr.save_mfdataset(ds_100km[0], [path_out+file for file in list_files])\n",
    "        xr.save_mfdataset(ds_100km_mask[0], [path_out+file.replace('SWE_SCA_POST', 'SWE_SCA_POST_MASK') for file in list_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine files to a single file\n",
    "Add also dates and rename/reorganize dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WY1999_00\n",
      "SWE_SCA_POST\n",
      "WY2000_01\n",
      "SWE_SCA_POST\n",
      "WY2001_02\n",
      "SWE_SCA_POST\n",
      "WY2002_03\n",
      "SWE_SCA_POST\n",
      "WY2003_04\n",
      "SWE_SCA_POST\n",
      "WY2004_05\n",
      "SWE_SCA_POST\n",
      "WY2005_06\n",
      "SWE_SCA_POST\n",
      "WY2006_07\n",
      "SWE_SCA_POST\n",
      "WY2007_08\n",
      "SWE_SCA_POST\n",
      "WY2008_09\n",
      "SWE_SCA_POST\n",
      "WY2009_10\n",
      "SWE_SCA_POST\n",
      "WY2010_11\n",
      "SWE_SCA_POST\n"
     ]
    }
   ],
   "source": [
    "path = '/mnt/lalandmi/equipes/C2H/HMASR/HMASR_100km_mask/'\n",
    "WY_list = ['WY'+str(year)+'_'+str(i).zfill(2) for i, year in enumerate(range(1999, 2017))]\n",
    "# ds_name_list = ['FORCING_POST', 'SD_POST',  'SWE_SCA_POST']\n",
    "ds_name_list = ['SWE_SCA_POST']\n",
    "\n",
    "for WY in WY_list:\n",
    "    print(WY)\n",
    "    for ds_name in ds_name_list:\n",
    "        print(ds_name)\n",
    "        ds = xr.open_mfdataset(path+WY+'/*'+ds_name+'.nc', parallel=True)\n",
    "        ds = ds.assign_coords(Day=pd.date_range(start=WY[2:6]+'-10-01', periods=ds.Day.size, freq='D'))\n",
    "        ds = ds.rename({'Longitude': 'lon', 'Latitude': 'lat', 'Day': 'time'}).transpose(\"Stats\", \"time\", \"lat\", \"lon\")\n",
    "        ds.to_netcdf(path+'HMA_SR_D_v01_100km_'+WY+'_'+ds_name+'.nc')\n",
    "        \n",
    "        # mask\n",
    "        ds = xr.open_mfdataset(path+WY+'/*'+ds_name+'_MASK.nc', parallel=True)\n",
    "        ds = ds.assign_coords(Day=pd.date_range(start=WY[2:6]+'-10-01', periods=ds.Day.size, freq='D'))\n",
    "        ds = ds.rename({'Longitude': 'lon', 'Latitude': 'lat', 'Day': 'time'}).transpose(\"Stats\", \"time\", \"lat\", \"lon\")\n",
    "        ds.to_netcdf(path+'HMA_SR_D_v01_100km_'+WY+'_'+ds_name+'_MASK.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_v3] *",
   "language": "python",
   "name": "conda-env-phd_v3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
